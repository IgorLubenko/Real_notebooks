{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упрощенный семантический анализатор**  \n",
    "**частоты повторений слов и фраз на основе выборки по ключевым словам из массива данных.  \n",
    "v.1.2**  \n",
    "\n",
    "\n",
    "Для свободного использования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автор: Игорь Лубенко   \n",
    "https://github.com/IgorLubenko  \n",
    "lubenkoiv@yandex.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Внимание! Важная информация для использования:**\n",
    "\n",
    "***1. Возможно использование данный парсер в двух вариантах:***  \n",
    "а) Полный: когда на вход подается файл с необработанными данными (например, data_jobs.csv) и алгорим выполняется с первого шага и до конца. В этом случае выполнение раздела 5 отключено (задокументировано);  \n",
    "б) Усеченный, когда на вход подается файл 'jobs_qa.csv' с ранее отфильтрованными данными, по которым затем выполняется семантический анализ. В этом случае шаги 2 - 4 следует пропустить и после выполнения шага 1 продолжить с шага №5;  \n",
    "\n",
    "\n",
    "***2. В алгоритме две ключевые развилки:***  \n",
    "  - выбор ключевых слов для отбора из массива данных - производится в разделе 4;  \n",
    "  - выбор колонки - источника для дальнейшего анализа количества повторяемых слов и фраз: производится в разделе 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Оглавление<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Загрузка-библиотек-и-начальные-операции\" data-toc-modified-id=\"Загрузка-библиотек-и-начальные-операции-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Загрузка библиотек и начальные операции</a></span></li><li><span><a href=\"#Загрузка-начальных-данных\" data-toc-modified-id=\"Загрузка-начальных-данных-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Загрузка начальных данных</a></span></li><li><span><a href=\"#Подготовка-данных\" data-toc-modified-id=\"Подготовка-данных-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Подготовка данных</a></span></li><li><span><a href=\"#ВНИМАНИЕ:-ключевой-раздел!-Создание-выборки-по-ключевым-словам\" data-toc-modified-id=\"ВНИМАНИЕ:-ключевой-раздел!-Создание-выборки-по-ключевым-словам-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>ВНИМАНИЕ: ключевой раздел! Создание выборки по ключевым словам</a></span></li><li><span><a href=\"#Загрузка-отфильтрованных-данных-(короткий-вариант,-если-фильтрация-по-словам-уже-выполнена)\" data-toc-modified-id=\"Загрузка-отфильтрованных-данных-(короткий-вариант,-если-фильтрация-по-словам-уже-выполнена)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Загрузка отфильтрованных данных (короткий вариант, если фильтрация по словам уже выполнена)</a></span></li><li><span><a href=\"#Исследование-данных\" data-toc-modified-id=\"Исследование-данных-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Исследование данных</a></span></li><li><span><a href=\"#Очистка-от-дубликатов\" data-toc-modified-id=\"Очистка-от-дубликатов-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Очистка от дубликатов</a></span></li><li><span><a href=\"#Формирование-данных-с-требованиями-к-кандидатам\" data-toc-modified-id=\"Формирование-данных-с-требованиями-к-кандидатам-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Формирование данных с требованиями к кандидатам</a></span></li><li><span><a href=\"#Очистка-данных:-удаление-ненужных-знаков-и-символов\" data-toc-modified-id=\"Очистка-данных:-удаление-ненужных-знаков-и-символов-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Очистка данных: удаление ненужных знаков и символов</a></span></li><li><span><a href=\"#Подсчет-количества-повторяющихся-слов\" data-toc-modified-id=\"Подсчет-количества-повторяющихся-слов-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Подсчет количества повторяющихся слов</a></span></li><li><span><a href=\"#Выделение-единичных-уникальных-слов-(токенизация-по-словам)\" data-toc-modified-id=\"Выделение-единичных-уникальных-слов-(токенизация-по-словам)-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Выделение единичных уникальных слов (токенизация по словам)</a></span></li><li><span><a href=\"#Нахождение-словосочетаний-(n-grams)\" data-toc-modified-id=\"Нахождение-словосочетаний-(n-grams)-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Нахождение словосочетаний (n-grams)</a></span></li><li><span><a href=\"#Вывод-результата-на-экран\" data-toc-modified-id=\"Вывод-результата-на-экран-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Вывод результата на экран</a></span></li><li><span><a href=\"#Результаты-стемминга\" data-toc-modified-id=\"Результаты-стемминга-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Результаты стемминга</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка библиотек и начальные операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявим языки для стемминга\n",
    "snowball_rus = SnowballStemmer('russian')\n",
    "snowball_eng = SnowballStemmer('english')\n",
    "\n",
    "# Объявим переменные для очистки от ненужных слов\n",
    "stop_words_rus = set(stopwords.words(\"russian\"))\n",
    "stop_words_eng = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прижелании берем ограничение на ширину количество знаков в столбцах таблиц\n",
    "# pd.set_option('display.max_colwidth', 0)  # Возврат к нормальному отображению - значение \"80\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка начальных данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка начальных данных из файла, содержащего необработанные данные\n",
    "path = 'C:\\\\Users\\\\Downloads\\\\'        # Путь к файлу \n",
    "initial_data = 'data_jobs.csv'      # Файл для тестирования\n",
    "df = pd.read_csv(path + initial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просмотр результата загрузки файла:\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приведем написание в столбце к нижнему регистру\n",
    "df['vacancy_name'] = df['vacancy_name'].str.lower()\n",
    "# Или:\n",
    "# df['vacancy_name'] = map(str.lower, df['vacancy_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приведем написание в столбце к нижнему регистру\n",
    "df['vacancy_description'] = df['vacancy_description'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приведем написание в столбце к нижнему регистру\n",
    "df['list_of_skill_names'] = df['list_of_skill_names'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в формат даты из текстового формата\n",
    "df['publication_day'] = pd.to_datetime(df['publication_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим столбец с непонятными значениями\n",
    "df.drop('Unnamed: 0', inplace=True, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим результат\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ВНИМАНИЕ: ключевой раздел! Создание выборки по ключевым словам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выберем стороки, содержащие в \"vacancy_name\", например, слова \"qa | q/a | тестировщик | quality\"\n",
    "# ВНИМАНИЕ: все буквы ключевых слов вводим строчными буквами (в ниженем регистре)\n",
    "\n",
    "jobs_qa = df.query('vacancy_name.str.contains(\"аналитик данных | data analyst | DA \")', engine='python').reset_index().copy()\n",
    "\n",
    "# Сохраним отфильтрованные данные для истории в файле:\n",
    "jobs_qa.to_csv('jobs_qa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка отфильтрованных данных (короткий вариант, если фильтрация по словам уже выполнена)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Это вариант загрузки данных для исследования напрямую из загружаемого файла \"jobs_qa.csv\"\n",
    "# path = 'C:\\\\Users\\\\.......\\\\'  # Необходимо полностью указать путь к файлу\n",
    "# file = 'jobs_qa.csv'\n",
    "# jobs_qa = pd.read_csv(path + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Исследование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_qa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим результат:\n",
    "jobs_qa.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Очистка от дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполним поиск дубликатов и их удаление\n",
    "print(\"Кол-во дубликатов до очистки:\", jobs_qa.duplicated().sum())\n",
    "print(\"Кол-во дубликатов в определенных столбцах до очистки:\", jobs_qa.duplicated(['vacancy_name', 'vacancy_description', 'list_of_skill_names', 'city_name']).sum())\n",
    "jobs_qa = jobs_qa.drop_duplicates(['vacancy_name', 'vacancy_description', 'list_of_skill_names', 'city_name'])\n",
    "print(\"Кол-во дубликатов после очистки:\", jobs_qa.duplicated(['vacancy_name', 'vacancy_description', 'list_of_skill_names', 'city_name']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Формирование данных с требованиями к кандидатам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сформируем два Series с данным столбцов со списком требований к кандидатам и описанием вакансии:\n",
    "skills = jobs_qa['list_of_skill_names']\n",
    "vacancy_description = jobs_qa['vacancy_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подставляем поочереди в строку ниже слова \"обязанности\", \"навыки\", \"требования\", \"условия\"\n",
    "# и выделяем строки, содержащие в заголовке инфо, подставляя в поиск нужное слово из списка выше:\n",
    "# ВНИМАНИЕ: какой Series подставим ниже (\"skills\" или \"vacancy_description\") - по тому и будет производиться выборка!\n",
    "data_splitted = vacancy_description.str.findall(r'<strong>\\w*требован\\w*:<\\/strong>(.+?)<strong>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Произведем преабразования, необходимые в дальнейшем\n",
    "data_splitted_explode = data_splitted.explode()\n",
    "data_splitted_explode = data_splitted_explode.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция обработки слов с строках:\n",
    "\n",
    "def data_to_string(text):\n",
    "    # Разделим слова и словосочетания по запятой\n",
    "    data_splited = text.str.split(',')\n",
    "\n",
    "    # Преобразуем список списков в простой список\n",
    "    data_splited = sum(data_splited, [])\n",
    "\n",
    "    return data_splited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Очистка данных: удаление ненужных знаков и символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция очистки данных\n",
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in (text[:]):                             # Альтернативный вариант: for x in (text[:][0])\n",
    "        new_text = re.sub('<.*?>', '', x)           # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punc.\n",
    "        new_text = re.sub(r'\\d+','',new_text)       # remove numbers\n",
    "        # new_text = new_text.lower()                 # lower case, .upper() for upper\n",
    "        new_text = re.sub(r'none', '', new_text)    # remove \"none\"\n",
    "        new_text = re.sub(r'qa', '', new_text)      # remove \"qa\"\n",
    "        new_text = re.sub(r'\\\"', '', new_text)      # remove \"\n",
    "        new_text = re.sub(r'^\\s+|\\s\\s+$', '', new_text) # Удаление пробелов в начале и конце фразы\n",
    "        new_text = re.sub(r'^\\s\\s+', '', new_text)  # Удаление пробелов в начале и конце фразы\n",
    "        new_text = re.sub(r'li', '', new_text)      # remove \"li\"\n",
    "        new_text = re.sub(r'ul', '', new_text)      # remove \"ul\"\n",
    "        new_text = re.sub(r'^p$', '', new_text)     # remove одиночное \"p\"\n",
    "        \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подсчет количества повторяющихся слов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция очистки и подсчета количества повторяющихся слов\n",
    "def count_words(text):\n",
    "    \n",
    "    # Очистка от предлогов и проч. ненужных и вспомогательных слов\n",
    "    without_stop_words = [word for word in text if not word in stop_words_rus] \n",
    "    data_cleared = [word for word in without_stop_words if not word in stop_words_eng] \n",
    "    \n",
    "    # Подсчитаем количество повторяющихся слов и фраз и сохраним результат в переменной\n",
    "    phrase_count = Counter(data_cleared)\n",
    "\n",
    "    # Выведем на печать наиболее часто встречающихся 100 словосочетаний\n",
    "    top_phrase_list = phrase_count.most_common(100)\n",
    "    top_phrase_list\n",
    "    \n",
    "    # Преобразуем в DataFrame\n",
    "    top_words = pd.DataFrame(columns=['Ключевые_слова', 'Количество'], data=top_phrase_list)\n",
    "\n",
    "    # Удалим строку с пробелом в значении\n",
    "    top_words = top_words.query('Ключевые_слова != \" \"')\n",
    "\n",
    "    # Удалим двойные пробелы между словами\n",
    "    top_words = top_words.replace(r'\\s\\s','',regex=True)\n",
    "\n",
    "    # Сгруппируем данные повторно, сохраним результат в DataFrame и выведем на экран:\n",
    "    top_words_groupped = top_words.groupby('Ключевые_слова')['Количество'].sum().sort_values(ascending=False)\n",
    "    top_words_groupped_df = pd.DataFrame(columns=['Количество'], data=top_words_groupped)\n",
    "\n",
    "    # Уберем одиночное слово \"тестирование\"\n",
    "    top_words_groupped_df = top_words_groupped_df.query('Ключевые_слова != \"тестирование\"')\n",
    "\n",
    "    top_words_groupped_df = top_words_groupped_df.reset_index()\n",
    "    print('Наиболее часто встречающиеся слова:')\n",
    "    display(top_words_groupped_df.head(20))\n",
    "    return(top_words_groupped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Построение гистограммы\n",
    "def hist_plot(text):\n",
    "    \n",
    "    fig = go.Figure(go.Bar(x=text['Ключевые_слова'].head(20),\n",
    "                           y=text['Количество'],\n",
    "                           text=text['Количество'].round(0),\n",
    "                           textposition='outside',\n",
    "                           textfont_size=12,\n",
    "                           marker_color= 'DarkSlateGrey'\n",
    "                           )\n",
    "                   )\n",
    "\n",
    "    fig.update_layout(title='ТОП-20 ключевых слов и фраз',\n",
    "                      title_font_size=14,\n",
    "                      height=700,\n",
    "                      autosize=False,\n",
    "                      width=1000,\n",
    "                      plot_bgcolor='#e2eeee',\n",
    "                      yaxis_title='Количество повторений',\n",
    "                      xaxis_tickangle=-40,\n",
    "                      xaxis_categoryorder= \"total descending\"\n",
    "                     )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выделение единичных уникальных слов (токенизация по словам)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция выделения единичных уникальных слов (токенизации по словам)\n",
    "def token():\n",
    "    skills_cleared_str = str(data_cleared)\n",
    "    \n",
    "    # Выделим слова и сохраним в переменной \"slkills_tokenizeds\" \n",
    "    skills_tokenizeds = []\n",
    "    for sentence in data_cleared:\n",
    "        skills_tokenizeds.append(nltk.word_tokenize(sentence))\n",
    "    \n",
    "    # Склеим в единый список полученные слова\n",
    "    skills_melted = []\n",
    "    for i in skills_tokenizeds:\n",
    "        skills_melted.extend(i)\n",
    "    \n",
    "    # Очистим от предлогов и вспомогательных слов\n",
    "    stop_words_rus = set(stopwords.words(\"russian\"))\n",
    "    stop_words_eng = set(stopwords.words(\"english\"))\n",
    "\n",
    "    skills_without_stop_words_ru = [word for word in skills_melted if not word in stop_words_rus] \n",
    "    skills_without_stop_words = [word for word in skills_without_stop_words_ru if not word in stop_words_eng]\n",
    "    \n",
    "    # Подсчитаем количество единичных уникальных слов (без использования стемминга)\n",
    "    skills_count_words_0 = Counter(skills_without_stop_words)\n",
    "    \n",
    "    # Выведем на экран первые n-слов (без использования стемминга)\n",
    "    skills_top = skills_count_words_0.most_common(30)\n",
    "    \n",
    "    # Преобразуем данные в DataFrame и выведем результат на экран\n",
    "    skills_top_df = pd.DataFrame(columns=['Ключевые_слова', 'Количество'], data=skills_top)\n",
    "\n",
    "    print('Выделение единичных уникальных слов (токенизация по словам):')\n",
    "    display(skills_top_df.head(20))\n",
    "    \n",
    "    # Построение гистограммы распределения:\n",
    "    fig = go.Figure(go.Bar(x=skills_top_df['Ключевые_слова'].head(20),\n",
    "                           y=skills_top_df['Количество'],\n",
    "                           text=skills_top_df['Количество'].round(0),\n",
    "                           textposition='outside',\n",
    "                           textfont_size=12,\n",
    "                           marker_color= 'DarkSlateGrey'\n",
    "                           )\n",
    "                   )\n",
    "\n",
    "    fig.update_layout(title='ТОП-20 ключевых слов и фраз',\n",
    "                      title_font_size=14,\n",
    "                      height=600,\n",
    "                      autosize=False,\n",
    "                      width=1000,\n",
    "                      plot_bgcolor='#e2eeee',\n",
    "                      yaxis_title='Количество слов и фраз',\n",
    "                      xaxis_tickangle=-45,\n",
    "                      xaxis_categoryorder= \"total descending\"\n",
    "                     )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нахождение словосочетаний (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция нахождения словосочетаний (n-grams)\n",
    "def n_grams_func():\n",
    "    # Преобразуем данные в строковый формат\n",
    "    data_cleared_str = str(data_splited)\n",
    "    \n",
    "    # Функция n-grams\n",
    "    def get_ngrams(text, n):\n",
    "        n_grams = ngrams(word_tokenize(text), n)\n",
    "        return [' '.join(grams) for grams in n_grams]\n",
    "    \n",
    "    # Определим количество слов в словосочетании\n",
    "    n = 6     \n",
    "\n",
    "    # Запустим функцию нахождения n-grams\n",
    "    ngrams_result = get_ngrams(data_cleared_str, n)\n",
    "    \n",
    "    # Повторно очистим текст от ненужных знаков (пунктуации)\n",
    "    ngrams_result = preprocess(ngrams_result)\n",
    "    \n",
    "    # Подсчитаем результат и найдем самые часто употребляемые слова и словосочетания\n",
    "    count_grams = Counter(ngrams_result)\n",
    "    count_grams = count_grams.most_common(100)\n",
    "\n",
    "    # Преобразуем полученные данные в DataFrame \n",
    "    skills_top_df = pd.DataFrame(columns=['Ключевые_слова', 'Количество'], data=count_grams)\n",
    "\n",
    "    # Удалим двойные пробелы между словами\n",
    "    # skills_top_df = skills_top_df.replace('^\\s+|\\s\\s+$', '',regex=True)\n",
    "\n",
    "    # Сгруппируем данные повторно, сохраним результат в DataFrame и выведем на экран:\n",
    "    n_grams_top_groupped = skills_top_df.groupby('Ключевые_слова', as_index=False)['Количество'].sum()\n",
    "    n_grams_top_groupped = n_grams_top_groupped.sort_values(by='Количество', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Уберем одиночное слово \"тестирование\"\n",
    "    n_grams_top_groupped = n_grams_top_groupped.query('Ключевые_слова != \"тестирование\"')\n",
    "    \n",
    "    # Выведем результат на экран\n",
    "    print('Словосочетания (n-grams), n=', n)\n",
    "    display(n_grams_top_groupped.head(20))\n",
    "    \n",
    "    # Построение гистограммы распределения:\n",
    "    fig = go.Figure(go.Bar(x=n_grams_top_groupped['Ключевые_слова'].head(20),\n",
    "                           y=n_grams_top_groupped['Количество'],\n",
    "                           text=n_grams_top_groupped['Количество'].round(0),\n",
    "                           textposition='outside',\n",
    "                           textfont_size=12,\n",
    "                           marker_color= 'DarkSlateGrey'\n",
    "                           )\n",
    "                   )\n",
    "\n",
    "    fig.update_layout(title='ТОП-20 ключевых слов и фраз',\n",
    "                      title_font_size=14,\n",
    "                      height=600,\n",
    "                      autosize=False,\n",
    "                      width=1000,\n",
    "                      plot_bgcolor='#e2eeee',\n",
    "                      yaxis_title='Количество слов и фраз',\n",
    "                      xaxis_tickangle=-45,\n",
    "                      xaxis_categoryorder= \"total descending\"\n",
    "                     )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод результата на экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запустим выполнение функций: \n",
    "data_splited = data_to_string(data_splitted_explode)\n",
    "data_cleared = preprocess(data_splited)\n",
    "top_words_groupped_df = count_words(data_cleared )\n",
    "hist_plot(top_words_groupped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_grams_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результаты стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция стемминга (Russian and English)\n",
    "def stemming(words):\n",
    "    # Очистка от предлогов и проч. ненужных и вспомогательных слов\n",
    "    without_stop_words = [word for word in words if not word in stop_words_rus] \n",
    "    data_cleared = [word for word in without_stop_words if not word in stop_words_eng] \n",
    "    \n",
    "    new = []\n",
    "    \n",
    "    \n",
    "    stem_words_rus = [snowball_rus.stem(x) for x in data_cleared]\n",
    "    stem_words = [snowball_eng.stem(x) for x in stem_words_rus]\n",
    "    new.append(stem_words)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполним функцию стемминга и сохраним результат в переменной\n",
    "skills_without_stop_words_stem = stemming(data_cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем получившийся список списков в простой список\n",
    "skills_without_stop_words_stem = sum(skills_without_stop_words_stem, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчитаем количество уникальных слов в списке \n",
    "skills_count_words = Counter(skills_without_stop_words_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выведем на экран первые n-слов слов списка ранжировав их по убыванию\n",
    "skills_count_words = skills_count_words.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в DataFrame и выведем результат на экран\n",
    "skills_top_stemming = pd.DataFrame(columns=['Ключевые_слова', 'Количество'], data=skills_count_words)\n",
    "print('Результаты стемминга:' )\n",
    "skills_top_stemming.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "279.965px",
    "width": "185.972px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Оглавление",
   "title_sidebar": "Содержание",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
